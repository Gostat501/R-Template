---
title: "Statistics Templates"
author: "Go Ito"
date: "August 9th, 2019"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
    latex_engine: xelatex
    number_sections: no
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{enumitem}
- \usepackage{amsmath,amssymb,amsthm, amsfonts}
- \usepackage{fontspec}
- \usepackage{multirow}
- \geometry{left=1in,top=0.75in,right=1in,bottom=.75in,textheight=8.5in, headheight=0.25in,headsep=0.125in,
  footskip=0.25in, portrait, twoside=true}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \rhead{template}
- \lhead{template}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=5, fig.height=4, warning = FALSE, message= FALSE, out.width = '50%', comment=NA)
```

\newpage

# Package Installation

```{r}
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("alr3")
#install.packages("MASS")
#install.packages("ISLR")
#install.packages("class")
#install.packages("caret")
#install.packages("e1071")
#install.packages("leaps")
#install.packages("boot")
#install.packages("crossval")
#install.packages("resample")
#install.packages("glmnet")
#install.packages("pls")
#install.packages("splines")
#install.packages("gam")
#install.packages("akima")
#install.packages("tree")
#install.packages("randomForest")
```


# Generic library to use

```{r, results="hide"}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(readxl)
library(MASS)
library(car)
```

# Useful functions

```{r}
traindex = function(df, trainn = nrow(df)-nrow(df)/10){
  n = nrow(df)
  return(sample(1:n,trainn, replace=F))
}
```



\newpage

# Data Reading

## Built in data

```{r}
head(iris)
head(mtcars)
```

## .txt file

```{r}
cleaning = read.table("cleaning.txt", header=T)
head(cleaning)
```

## .csv file

```{r}
Heart = read.csv("Heart.csv", header=T)
head(Heart)
```

## .xls / .xlsx file

```{r, results="hide"}
library(tidyverse)
library(readxl)
```

```{r}
xlsxdata = read_excel("sample.xlsx")
xlsxdata
```

## .jason file

## SQL

```{r, results="hide"}
#library(dbplyr)
#library(RSQLite)
#dir.create("data", showWarnings = FALSE)
#download.file(url = "https://ndownloader.figshare.com/files/2292171",destfile = "data/portal_mammals.sqlite", mode = "wb")
#mammals = DBI::dbConnect(RSQLite::SQLite(), "data/portal_mammals.sqlite")
```

```{r}
#SQLdata = tbl(mammals, sql("SELECT year, species_id, plot_id FROM surveys"))
#head(SQLdata)
```

\newpage

# Exploratory Analysis

Check List:

- General distribution of the object, widely spread? Skewed? Any outliers?

- Check for NA, NaN, or any other missing values equivalent.


## Summary

- Check for the basic summary: mean, median, min, max, frequency.

- For frequency, we prefer thatt all category data have sufficient number of data,

- Check for the correlation among variables.


```{r}
summary(iris)
cor(iris[,-5])
```




## Histogram / Distributions

```{r}
# easiest
hist(iris$Sepal.Length)
#
plot(density(iris$Sepal.Length), col="red")

# much easier for exploratory analysis
ggplot(data = iris) + geom_bar(mapping = aes(x = Sepal.Length))
ggplot(data=iris) + geom_histogram(mapping = aes(x=Sepal.Length))
ggplot(data=iris, mapping = aes(x=Sepal.Length, colour = Species)) + geom_freqpoly()
ggplot(Heart) + geom_density(mapping=aes(x=MaxHR, color=as.character(Sex)))

```


- Which values are most common among which cateogry?

- Which values are rare, or odd? Could it be an outlier, or mis-interpreted?

- Any unusual patters? Can you explain it?

- Why setosa tends to have smaller Sepal.Length than virginica?

## Boxplots

```{r}
ggplot(data = iris) + geom_boxplot(mapping = aes(x=Species, y=Sepal.Length, fill=Species))
```

## Plots of two variables

```{r}
ggplot(data=iris) + geom_point(aes(x=Sepal.Length, y=Sepal.Width, colour=Species))
```

## Interaction Plots

```{r}
ggplot(data=iris) + aes(x = Sepal.Length, y = Sepal.Width, colour = Species) +
  geom_point(color = "grey") +
  geom_smooth(method = "lm")
```


- Often used in time series

```{r}
interaction.plot(x.factor = iris$Sepal.Length, trace.factor = iris$Species, response = iris$Sepal.Width, lwd=2, col = 1:3)
```

\newpage

# Data Analysis, Prediction, and Classification

## Simple Linear Regression

### Assumption Check

- 1. Average is 0?

- 2. Standarized residual (more informative when leverage points exist because errors can show const var while residuals don't / how many estimated std deviations any point away from the fitted regression model / if outside -2 to 2, outlier), check for constant variance primarily here!

- 3. Normality (straight line) holds? For each $x$, see if corresponding $y$ follow normal distributions where mean is fitted line.

- 4. Any outliers, leverage points(influential to fitted model / how predicted y change if removed / bigger than $4/n$), outside Cook's distance ($D_i=\frac{r_i^2}{2}*\frac{h_ii}{1-h_ii}$ / $D_i>4/(n-2)$?

- If any of the assumptions violated, any further inferences are invalidated.

```{r}
model1 = lm(Sepal.Width~Sepal.Length, data=iris)
plot(model1)

head(lm.influence(model1)$hat)
```

### Statistical Inference

- Shape of the plots, linear? quadratic? exponential?

```{r}
plot(Sepal.Width~Sepal.Length, data=iris);abline(model1)

# Equivalently this, but much more complicated
predicted1 = data.frame(Sepal.Length=iris$Sepal.Length, predicted = predict(model1, iris))
ggplot(data=iris) + geom_point(aes(x=Sepal.Length,y=Sepal.Width))+ geom_line(color='red',data = predicted1, aes(x=Sepal.Length, y=predicted))
```

- Coefficients significant? P-value? Standard Error?

- Positive? Negative? Its strength?

- "One unit increase in X results in $\beta_1$ much increase in Y"

- $\hat{\beta_1} = r*\frac{S_Y}{S_X}=\frac{\sum^n(X-\bar{X})(Y-\bar{Y})}{\sum^n(Y-\bar{Y})^2}$

- $\hat{\beta_0} = \bar{Y}-\hat{\beta_1}\bar{X}$

- $e\sim N(0,\sigma^2)$

- $S^2 = \frac{\sum^n(Y-\hat{Y})^2}{n-2}$

- $se(\hat{\beta_0})^2=S^2 (\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum^n(x-\bar{x})^2})$

- $se(\hat{\beta_1})^2=\frac{S^2}{\sum^n(X-\bar{X})^2}$

- $SSE=\sum^n(y-\hat{y})^2$

- $SSR=\sum^n(\hat{y}-\bar{y})^2$

- $SST=\sum^n(y-\bar{y})^2$

- $R^2= 1-\frac{SSE}{SST}$

- $F=\frac{SST-SSE/1}{SST/n-2}=t^2=(\frac{\hat{\beta_1}}{se(\hat{\beta_1})})^2$

```{r}
summary(model1)
var(iris$Sepal.Length)
```

- Confidence Interval: $\hat{y}\pm t_{n-2}*S\sqrt{\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum^n(x-\bar{x})^2}}$

- CI ex) $\hat{y}\pm2.06*0.4343 \sqrt{\frac{1}{150}+\frac{(5.0-5.843333)^2}{149*0.6856935}}$

- Prediction Interval: $\hat{y}\pm t_{n-2}*S\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum^n(x-\bar{x})^2}}$

- PI ex) $\hat{y}\pm2.06*0.4343 \sqrt{1+\frac{1}{150}+\frac{(5.0-5.843333)^2}{149*0.6856935}}$

### ANOVA SLR

|Variation|df|SS|MS|F|
|---|---|--|--|---|-----------|
|Regression|1|SSR|SSR/1|SSR/(SSE/(n-2))|
|Residual/Error| n-2| SSE| SSE/n-2||
|Total|n-1|SST|||

```{r}
anova(model1)
```

\newpage

### Transformation

#### Inverse response transformation 

- $g^{-1}(y)=\beta_0+\beta_1x+\epsilon$, e.g. $g(y)=exp(y), g^{-1}(y)=log(y)$, $g(y)=y^\lambda, g^{-1}(y)=y^{1/\lambda}$, make sure response only!

- Pick $\lambda$ that has the lowest RSS/SSE.

```{r}
library(alr3)
inverseResponsePlot(model1,key=TRUE)
```

#### Box-cox transformation

- Try to make vairables close to normally distributed. For SLR, maximize likelihood = minimize $SSE(\lambda)=\sum(y^\lambda-\hat{\beta_0}-\hat{\beta_1}x)^2$. Don't assume normality of $x$.

- Pick the $\lambda$ = `Rounded Pwr`

```{r}
library(MASS)
model1bc = powerTransform(model1)
summary(model1bc)
```

#### Log transformation

- Take logarithm on response or predictors or both. $\log(y_2/y_1)=\beta_1*\log(x_2/x_1)$. "One percentage change in X results in $\beta_1$ percentage change in Y".

- Transform variables according to `Rounded Pwr`, e.g. `Y1=log(Y1)`, `Y2=Y2^1`.

- `Rounded Pwr = 0` means log transform.

- e.g. if `Rounded Pwr = 0.5` for `Y3`, `Y3=sqrt(Y3)`

```{r}
model1pt = powerTransform(cbind(iris$Sepal.Length,iris$Sepal.Width)~1) 
summary(model1pt)
```

\newpage

### Weighted Least Square

- When constant variance is violated, assigning a reasonable weigth to each variance could fix the problem.

- Assign inversely proportional weights to the corresponding variances.

- $SSE = \sum^n(Y-(\hat{\beta_0}+\hat{\beta_1}X))^2$

- $WSSE = \sum^nw_i(Y-(\hat{\beta_0}+\hat{\beta_1}X))^2$ with $\epsilon\sim N(0,\sigma^2/w_i)$

- Then, $var(\sqrt{w_i}\epsilon_i)=\sigma^2$

- The weights are assumed to known, so the estimated weights are used. Thus, this method works when the weights can be estimated precisely relative to one another.

- Sensitive to outliers, and possibly increase the influence of them.

```{r}
model2 = lm(Rooms~Crews, data=cleaning)
model2w = lm(Rooms~Crews, weights=1/StdDev^2, data=cleaning)

plot(cleaning$Crews, model2$residuals, main="SLR");abline(h=0);plot(cleaning$Crews, model2w$residuals*(1/cleaning$StdDev), main="WSLR");abline(h=0)
```

\newpage

## Multiple Linear Regression

### Assumptions

- The response variable and predicotrs are linearly related.

- Error terms are normally distributed.

- Error terms have a constant variance.

- Check the outliers, leverage points, and influential points.

- Check if the predictors are highly correlated (multicolliniearity).

- Check the diagonal elements of the hat matrix $\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$. If $h_{ii} > 2*\bar{h}=2*\frac{p+1}{n}$, the point is consider to be a leverage points for MLR.

- Standarized residual $r_i = \frac{\hat{e_i}}{S\sqrt{1-h_{ii}}}$ where $S^2=\frac{SSE}{n-(p+1)}$. If $r_i$ is outside $(-2,2)$, it's considered to be an outlier.

- Likewise, check for Cook's distance $D_i$. If greater than $4/(n-2)$, the point is an influential point for MLR.

- The diagnosis plots show if the entire model is valid.

```{r}
m = lm(MaxHR~Age+Chol+RestBP+Oldpeak, data=Heart)
pairs(MaxHR~Age+Chol+RestBP+Oldpeak, data=Heart)
plot(m)
```

```{r}
stdres1 = rstandard(m)
lev1 = hatvalues(m)
cookd1 = cooks.distance(m)

# outliers
which(abs(stdres1)>2)
# leverage ( hii > 2*(p+1)/n)
which(lev1 > 2*(4+1)/nrow(Heart))
# influential points (Di > 4/(n-2))
which(cookd1 > 4/(nrow(Heart)-2))
```

- When strong correlations exist among the predictor variables, the following issues may arise:

- 1. F-test results will be highly significan, when very few predictors are significant.

- 2. Some of the coefficients in the model show the opposite sign than expected.

- Variance Inflation Factor (VIF) : $\frac{1}{1-R_j^2}$, where $R_j^2$ denote the value of $R^2$ obtained from the regression of $x_j$ on the other$x$'s. Note that $var(\beta_j)=\frac{1}{1-R_j^2}*\frac{\sigma^2}{(n-1)S_{x_j}^2}$. If $VIF_j>5$, then $\hat{\beta_j}$ is poorly estimated due to multicollinearity.

- There are several ways to handle multicollinearity. One way is to delete the redundant predictors (highly correlated predictors).

- Another way to handle multicollenearity is to make the dataset uncorrelated i.e. linearly independend. The method is called Principle Component Analysis.

```{r}
library(car)
vif(m)
```


### Statistical Inference

- $\mathbf{Y} = \mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}$

- $SSE = (\mathbf{Y}-\mathbf{X}\hat{\mathbf{\beta}})^T(\mathbf{Y}-\mathbf{X}\hat{\mathbf{\beta}})=||\mathbf{Y}-\mathbf{X}\hat{\mathbf{\beta}}||^2$

- $\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}$

- $\mathbf{X}=(1,X_1,X_2,\dots,X_p)^T$

- $S^2=\frac{SSE}{n-p-1}$

- $T_i=\frac{\hat{\beta_i}}{se(\hat{\beta_i})}\sim t_{n-p-1}$ for $H_0:\beta_i=0$

- $R^2 = 1-\frac{SSE}{SST}$, but always increase as $p$ increase. 

- $R^2_{adj} = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$

- $F=\frac{(SST-SSE)/p}{SSE/(n-p-1)}\sim F_{p,(n-p-1)}$ for $H_0:\beta_1=\dots=\beta_p=0$

```{r}
summary(m)

# or
Y = Heart$MaxHR
X = cbind(1,Heart$Age, Heart$Chol, Heart$RestBP, Heart$Oldpeak)
beta = solve(t(X)%*%X)%*%t(X)%*%Y;beta
```

\newpage

### Model Diagnosis : Added Variable Plot

- Added variable plot enable us to visually assess the effect of each predictors, having adjusted for the effects of other predictors.

- In stead of the model $\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\epsilon$, consider $\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{Z}\mathbf{\alpha}+\epsilon$.

- $\mathbf{Z}=\mathbf{X}\mathbf{\delta}+\mathbf{\epsilon}$ and $n\times 1$ vector. If the model with $\mathbf{Z}$ fits better to the data, then the added variable plot should produce points randomly scattered around a line through thte origin with slope $\hat{\alpha}$

```{r}
library(car)
m = lm(iris$Sepal.Width~iris$Sepal.Length+iris$Petal.Length+iris$Petal.Width)
par(mfrow=c(2,2))
avPlot(m,variable=iris$Sepal.Length, ask=F)
avPlot(m,variable=iris$Petal.Length, ask=F)
avPlot(m,variable=iris$Petal.Width, ask=F)
par(mfrow=c(1,1))
```

\newpage

### ANOVA MLR

|Variation|df|SS|MS|F|
|-----|---|---|---|---|-----------|
|Regression|p|SSR|SSR/p|(SST-SSE)/p/(SSE/(n-2))|
|Residual/Error| n-p-1| SSE| SSE/n-p-1||
|Total|n-1|SST|||

```{r}
anova(model2)
```

### Nested Model and Partial F-test

$$\begin{aligned}
H_o &: \beta_1=\dots=\beta_k=0 \quad (\text{k < p i.e. reduced model})\\
&\text{vs.}\\
H_a &: \beta_1=\dots=\beta_p=0 \quad (\text{i.e. full model})\\
\end{aligned}$$

- $F = \frac{SSE_{reduced}-SSE_{full}/(df_{reduced}-df_{full})}{SSE_{full}/df_{full}}=\frac{SSE_{reduced}-SSE_{full}/k}{SSE_{full}/df_{full}}$

- Although here the term "reduced" and "full" are used, make sure that "full" could already be a reduced model i.e. $n-p-1 \ge df_{full}$.

```{r}
model2reduced = lm(MaxHR~Age+RestBP+Oldpeak, data=Heart)
anova(model2,model2reduced)
```

- Here, F-stats is not significant i.e. failed to reject null, reduced model is better.

\newpage

### ANCOVA (Analysis of Covariance)

- Suppose we have a categorical variable with K levels. ANCOVA allows a categorical variable to be included in a linear model. Technically speaking, ANOVA is a sub-technique of ANCOVA where we have a hidden categorical variable with only 1 level.

- There could be multiple categorical variable. If that's the case, the number of levels would be $K_1\times K_2$, and might require more sample size.

- Use variable selections (e.g. partial F-test) to reduce the number of predictors.

- $Y=\beta_0+\beta_1x+\beta_2 d+\beta_3(d\times x)+\epsilon$ if $d\in \{0,1\}$

- $SSB=\sum^k_{i=1}\sum^{n_k}_{j=1}(\bar{Y_i}-\bar{\bar{Y}})^2$, similar to SSE

- $SSW = \sum^k_{i=1}\sum^{n_k}_{j=1}(Y_{ij}-\bar{Y_i})^2$, similar to SSR

- $SST = \sum^k_{i=1}\sum^{n_k}_{j=1}(Y_{ij}-\bar{\bar{Y}})^2$

- $H_o: \mu_1=\mu_2=\dots=\mu_K$ vs. $H_a:$ at least one of the group means is different


|Variation|df|SS|MS|F|
|-----|---|---|---|---|-----------|
|Between|k-1|SSB|SSB/(k-1)|SSB/(k-1)/(SSW/(n-k))|
|Within| n-k| SSW| SSW/n-k||
|Total|n-1|SST|||

```{r}
ggplot(data = Heart) + geom_boxplot(mapping = aes(x=factor(Sex), y=MaxHR, fill=as.character(Sex)))

model3 = lm(MaxHR~Age*Sex, data=Heart)
# Same as lm(MaxHR~Age+Sex+Age:Sex)
summary(model3)
ggplot(data=Heart) + aes(x=Age, y=MaxHR,colour=as.character(Sex))+ geom_point() + geom_smooth(method="lm")
```


\newpage

### Polynomial Regression

- $Y = \beta_0 + \beta_1 x +\beta_2 x^2+\dots+\beta_hx^h+\epsilon$

- Pick the degree that gives the lowest MSE and higest R square adjusted.

- Be careful of over-fitting.

```{r}
Y = iris$Sepal.Width
X = iris$Sepal.Length
polym1 = lm(Y~X)
summary(polym1)
qplot(X,polym1$fitted.values, geom=c("point","smooth"))

polym2 = lm(Y~X+I(X^2))
summary(polym2)
qplot(X,polym2$fitted.values, geom=c("point","smooth"))

polym3 = lm(Y~X+I(X^2)+I(X^3))
summary(polym3)
qplot(X,polym3$fitted.values, geom=c("point","smooth"))

polym4 = lm(Y~X+I(X^2)+I(X^3)+I(X^4))
summary(polym4)
qplot(X,polym4$fitted.values, geom=c("point","smooth"))

polym5 = lm(Y~X+I(X^2)+I(X^3)+I(X^4)+I(X^5))
summary(polym5)
qplot(X,polym5$fitted.values, geom=c("point","smooth"))

#MSEs
polyMSE = c(sqrt(sum(polym1$residuals)^2)/148,sqrt(sum(polym2$residuals)^2)/147,sqrt(sum(polym3$residuals)^2)/146,sqrt(sum(polym4$residuals)^2)/145,sqrt(sum(polym5$residuals)^2)/144)
polyMSE
plot(1:5, polyMSE, type="b", main="degree vs residual", ylab="MSE")
```

\newpage

### Model Selection Criteria

- When multicollinearity among the predictor variables are observed, model selection is one way to resolve it.

- Make sure to check all the assumptions are met before starting model selection.

- Goodness of fit criteria:

1. Adjusted R-square: $R_{adj}^2=1-\frac{SSE/(n-p-1)}{SST/(n-1)}=1-\frac{(1-R^2)(n-1)}{(n-p-1)}$. We don't use R-square because it automatically increase as the number of predictors increase.

2. Akaike Information Criterion (AIC): Smaller the better. Reward for a good fit + penalty for complexity. $AIC=n*\log(\frac{SSE}{n})+2p$

3. AIC corrected: Greater penality. Smaller the better. $AIC_C=AIC + \frac{2p(p+2)(p+3)}{n-p-1}$

4. Bayes Information Criteria (BIC): Greater penalty than AIC when $\log(n)>2$, thus favors simpler model than $AIC$. As the sample size n increase, the probability that BIC choose the correct model becomes 1. For smaller n, BIC choose too simple model hence biased. $BIC = n*\log(\frac{SSE}{n}) + \log(n)*p$

5. Mean Square Error: When test data is given, MSE is the most reliable measurement for choosing the best model. Combine with CV.

### Subset Selection: Best Subset Model

- For p predictors, we have $\sum^p_{k=1} {p\choose k}$ possible subset models.

1. Start with $k = p$ i.e. full model, fit the model.

2. $k = p-1$. Fit all $p\choose p-1$ models, keep the winner among $p\choose p-1$ with higest $R^2$. Here it's $R^2$ because we are comparing the models with same number of predictors.

3. $k = p-2$, keep the winner.

4. Repeat until $k=1$.

5. Choose the best model among all winners. Use the criteria other than $R^2$.

-----

#### R: Best Subset Model

\ 

```{r}
library(leaps)
X = cbind(iris$Sepal.Length, iris$Petal.Length, iris$Petal.Width)
b = regsubsets(X, iris$Sepal.Width)
bs = summary(b);bs

om1 = lm(Sepal.Width~Petal.Length,data=iris)
om2 = lm(Sepal.Width~Sepal.Length+Petal.Length,data=iris)
om3 = lm(Sepal.Width~Sepal.Length+Petal.Length+Petal.Width,data=iris)

n = nrow(iris)

p=1
AIC1 = extractAIC(om1,k=2)[2]
AICc1 = extractAIC(om1,k=2)[2] + 2*(p+2)*(p+3)/(n-p-1)
BIC1 = extractAIC(om1, k=log(n))[2]

p=2
AIC2 = extractAIC(om1,k=2)[2]
AICc2 = extractAIC(om1,k=2)[2] + 2*(p+2)*(p+3)/(n-p-1)
BIC2 = extractAIC(om1, k=log(n))[2]

p=3
AIC3 = extractAIC(om1,k=2)[2]
AICc3 = extractAIC(om1,k=2)[2] + 2*(p+2)*(p+3)/(n-p-1)
BIC3 = extractAIC(om1, k=log(n))[2]

AIC = c(AIC1,AIC2,AIC3)
AICc = c(AICc1, AICc2, AICc3)
BIC = c(BIC1, BIC2, BIC3)

data.frame(Radj2 = bs$adjr2,AIC,AICc,BIC)
```

-----

\newpage

#### R: Best Subset Model (Alternative)


- Black represents that the variables are selected. Pick the variables that highlighted on the very top.

\ 

```{r}
library(ISLR)
Hitters=na.omit(Hitters)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)

par(mfrow=c(1,1))
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="bic")
```


\newpage

### Subset Selection: Stepwise Regression

#### Forward Stepwise

1. Start with null model (intercept only)

2. Fit model with $k=1$. Choose the best among $p$ models based on $R^2$.

3. Add another vairbale to the previous model, keep the best model.

4. Repeat until $k=p$.

5. Choose the best among the $p$ candidate models using AIC, BIC etc...

---

#### R: Forward AIC

\ 

```{r}
m = lm(Sepal.Width~1, data=iris)

forwardAIC = step(m, scope = list(lower=~1,
upper=~Sepal.Length+Petal.Length+Petal.Width,data),
direction = "forward", data=iris)
```

\newpage

#### R: Forward BIC

\ 

```{r}
forwardBIC = step(m, scope = list(lower=~1,
upper=~Sepal.Length+Petal.Length+Petal.Width,data),
direction = "forward", data=iris, k=log(n))
```


\newpage



#### Backward Stepwise

1. Start with full model.

2. Fit all models with $k=p-1$, pick the best model.

3. Reduce another vriable from 2., pick the best model.

4. Repeat until $k=1$.

5. Choose the best among the $p$ candidate models using AIC, BIC etc...

---

#### R: Backward AIC

\ 
```{r}
m = lm(mpg~disp+hp+drat+wt+qsec,data=mtcars)
backAIC = step(m,direction = "backward", data=mtcars)
```

\newpage

#### R: Backward BIC

\ 
```{r}
backBIC = step(m, direction="backward", data=mtcars, k=log(n))
```

\newpage

#### R: Forward and Backward Selection (Alternative)

\ 

```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")

# Choosing best number of predictors w/ CV, based on MSE.

k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}


for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
  }
}
mean.cv.errors=apply(cv.errors,2,mean)
plot(mean.cv.errors,type='b')
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19);coef(reg.best,11)
```


\newpage

### Shrinkage: Regularization

- Fit a model involving all p predictors, but the estimated coefficients are shrunken toward zero. This shirknkage has the effect of reducing variance and can also perfrom variable selection.

- For both shrinkage methods below, it is recommended to use CV to choose the best tuning parameter.

#### Ridge Regression

- Instead of minimizing loss function $SSE=(Y-X\hat{\beta})^T(Y-X\hat{\beta})$, minimize the loss function with L2 penalty term: 

- Minimize $(Y-X\hat{\beta})^T(Y-X\hat{\beta})+\lambda \hat{\beta}^T\hat{\beta}=SSE+\lambda ||\hat{\beta}||^2_2$ where $\lambda \ge 0$

- The coefficients generated by Ridge regression tends to be similar in value (absolute value-wise) as the tuning parameter $\lambda$ increases. This is because the penalty term uses Euclidean distance, or L2 norm. When the tuning parameter is too large, all coefficients goes down to 0.

- Ridge regression coefficiet estimate can change substantially when multiplying a given predictor by a constant, due to L2 norm part. Thus, it is best to apply Ridge regression after standarizing the predictors using the formula: $\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum^n_{i=1}(x_{ij}-\bar{x}_j)^2}}$

#### LASSO regression

- Minimize $(Y-X\hat{\beta})^T(Y-X\hat{\beta})+\lambda \mathbf{1}^T|\hat{\beta}|=SSE+\lambda ||\hat{\beta}||_1$ where $\lambda \ge 0$

- The coefficients generated by LASSO regression tends to be reduced down to 0 as the tuning parameter increases. This is because the penalty term uses Manhattan distance, or L1 norm. Thus, this shrinkage method can perform as variable selection.

- We say that the lasso yeilds sparse models i.e. the models that involve only a subset of variables, or contains many zeros.

#### Elastic Net

- The combination of LASSO and Ridge.

- Minimize $SSE+\lambda (\frac{(1-\alpha)}{2}||\hat{\beta}||^2_2+\alpha||\hat{\beta}||_1)$ where $\lambda \ge 0$, $\alpha \in (0,1)$


\newpage

#### R: Shrinkage

- Ridge: $\alpha = 0$

- LASSO: $\alpha = 1$

- Elastic Net: $\alpha \in (0,1)$

- Make sure to use matrix / vector for the inputs. Sparse matrix from `library(Matrix)` is supported.

---

#### R: Ridge Regression

\ 

```{r}
library(glmnet)
library(ISLR) # For Hitters dataset
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary


set.seed(1)
train=traindex(x)
temp = 1:nrow(x)
test=temp[-train]
y.test=y[test]
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x[train,],y[train], alpha=0,lambda=grid, thresh=1e-12)

# lambda = 4
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)

# lambda = 0
ridge.pred=predict(ridge.mod,s=0,newx=x[test,])
mean((ridge.pred-y.test)^2)

# When lambda = 0, same as lm
lm(y~x, subset=train)

ridge.mod=glmnet(x[train,],y[train], alpha=0,lambda=0, thresh=1e-12)
predict(ridge.mod,s=0,exact=T,type="coefficients")


# Cross Validation to choose lambda
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.mod=glmnet(x[train,],y[train], alpha=0,lambda=bestlam, thresh=1e-12)
ridge.pred=predict(ridge.mod, newx=x[test,])
mean((ridge.pred-y.test)^2)

```

\newpage


#### R: LASSO Regression

\ 

```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)

bestlam=cv.out$lambda.min
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=bestlam)
lasso.pred=predict(lasso.mod,newx=x[test,])
mean((lasso.pred-y.test)^2)

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,];lasso.coef[lasso.coef!=0]
```



\newpage

### Dimension Reduction: Principle Component Analysis

- Project the p predictors into M-dimensional subspace, where M < p. This is achieved by computing M different linear combinations of the variables. Then, M projections are used as predictor to fit a linear regression model by least squares.

- Before applying PCA, normalize the dataset.

- The first principle component is the linear combination of the variables with the largest variance so that the dataset is easily distinguishable e.g. LDA or grouping would perform well.

- The second principle component is also the linear combination of the vairables with the largest variance, subject to being uncorrelated with the first one i.e. perpendicular / independent of the first one, and third PC, forth PC, and so on.

- Drawback: the directions / new dimensions identified by PCA, i.e. the way linear combinations are created, is unsupervised way since the response Y is not used to help determine the PCA directions. Thus, there is no gurantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.

- Regression on data with PCA applied is called principle component regression.

- Mathematically, we have the following optimization problem. Solving this using the method of Lagrange multiplier, it reveals that the PCA  coefficients of the linear combinations are the eigenvectors of the variance-covariance matrix of a dataset with the eigenvalues equal to the Lagrange multipliers.

- A dataset with all PC's applied has a variance-covariance matrix whose non-diagonal entires are zero. That is, the all variables are uncorrelated.

$$\text{min}\quad \mathbf{W}^T\mathbf{\Sigma}\mathbf{W}\\
s.t. \mathbf{W}^T\mathbf{W}=1$$

---

#### R: PCA

```{r}
mtcars_temp = subset(mtcars,select=c(mpg,wt, disp, qsec))
mtcars_c = scale(mtcars_temp) # Normalize
out_pca = princomp(mtcars_c) # PCA

summary(out_pca)
W = out_pca$loadings;W # Linear combinations

# Dimension Reduction to PC1
X = as.matrix(mtcars_c)
XW = X%*%W ; XW[,1]

# Covariance Matrix
var(X)
round(var(XW),7)
```

\newpage

#### R: Principle Component Regression

- Pick `ncomp` that minimize MSEP.

\ 

```{r}
library(pls)
train.i = traindex(mtcars_temp)

mtcars_train = mtcars_temp[train.i,]
mtcars_test = mtcars_temp[-train.i,2:4]
y_test = mtcars_temp[-train.i,1]

pcr.fit=pcr(mpg~., data=mtcars_train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")

pcr.pred=predict(pcr.fit,mtcars_test,ncomp=1)
mean((pcr.pred-y_test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=1)
summary(pcr.fit)
```

\newpage

### Spline

- The truth is that association between variables are hardly ever linear. However, polynomial regression often cannot capture all features and associations among variables. To achieve this, fit linear regression piece-wise, and fit different polynomials for each pieces, then smooth the entire curve. Thus, spline works on one continuous predictor.

- Keep bias-variance trade off in mind. Linear model is highly biased, but if we attempt to increase the flexibility of the model, the bias decreases but variability increases. Add more flexibility until the bias is low enough yet the variability is not too high.

- The junctions of pieces is called "knot". 

- Step functions are used to divide the predictor variables and add knots. `cut` command divides the predictor variables into K many equal length intervals and assign each observation into the appropriate interval. Use `break` option to make your own cut if desired.

- Spline is the combine step functions and polynomial regression. One draw back is that piecewise polynomials can create a discontinuity at knots. Thus, we smooth the curve.

- `bs(X, knot)`: Any degree splines e.g. Linear splines and cubic splines (piece-wise cubic polynomials). A cubic spline with K knots has K+4 df.

- `ns(X, df)`: Natural cubic splines. A natural cubic spline extrapolates linearly beyond the boundary knots. This adds 4  extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline. Thus, natural splines with K knots has K df.

- `s(X, df)`: Smoothing splines, fit model via minimize $SSE+\lambda\int g''(t)dt$. Smoothing splines avoid the knot-selection issue, leaving a
single $\lambda$ to be chosen. Here, `smooth.spline()` function fit a smoothing spline, and we choose df instead of $\lambda$.

- `lo(X, span)`: Local regression spline, also called loess. We fit separate linear fits over the range of the predictor variable by weighted least squares. Highly flexible. Use `loess()` function as an alternative.

---

#### R: Step function

```{r}
library(splines)
m = lm(Sepal.Width~cut(Sepal.Length, 3), data=iris)
summary(m)
```

\newpage

#### R: Spline

\ 

```{r}
library(splines)
grids = seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by=0.01)

m = lm(Sepal.Width~bs(Sepal.Length, knots=c(4.3, 5.5, 6.7, 7.9)), data=iris)
summary(m)
pred=predict(m,newdata=list(Sepal.Length=grids),se=T)
plot(Sepal.Width~Sepal.Length, data=iris);lines(grids,pred$fit,lwd=2)

fit=smooth.spline(iris$Sepal.Length,iris$Sepal.Width,cv=T);fit$df
plot(Sepal.Width~Sepal.Length, data=iris);lines(fit,col="red",lwd=2)

m=lm(Sepal.Width~ns(Sepal.Length,df=16),data=iris)
pred=predict(m,newdata=list(Sepal.Length=grids),se=T)
plot(Sepal.Width~Sepal.Length, data=iris);lines(grids, pred$fit,col="red",lwd=2)

```

\newpage

### Generalized Additive Models

- Allows flexible non-linrarities in multiple variables, but retains the additive structure of linear models. For instance, We can fit multiple splines or local regression.

- Use `anova()` to compare models. In the case below, model2 is the best.

---

```{r}
library(gam)
m1 = gam(Sepal.Width~s(Sepal.Length,df=16), data=iris)
m2 = gam(Sepal.Width~s(Sepal.Length,df=16)+s(Petal.Width,3), data=iris)
m3 = gam(Sepal.Width~s(Sepal.Length,df=16)+s(Petal.Width,3)+Petal.Length, data=iris)
anova(m1,m2,m3,test="F")
summary(m2)
# training MSE
mean((iris$Sepal.Width-predict(m2,newdata=iris))^2)
library(akima)
plot(m2, se=TRUE, col="red")

grids1 = seq(min(iris$Sepal.Length),max(iris$Sepal.Length), length.out = 50)
grids2 = seq(min(iris$Petal.Length),max(iris$Petal.Length), length.out = 50)
grids3 = seq(min(iris$Petal.Width),max(iris$Petal.Width), length.out = 50)
irisdf = data.frame(Sepal.Length=grids1, Petal.Length=grids2,Petal.Width=grids3)
pred = predict(m2, newdata=irisdf)
plot(Sepal.Width~Sepal.Length, data=iris);lines(grids1, pred,col="red",lwd=2)
```

\newpage

### Logistic Regression

- Probabilistic, supervised learning, classification

- Linear regression produce a result ranged in $(-\infty,\infty)$, which is not appropriate here. Thus, when binary outcome is present, logistic regression is appropriate.

- $\theta(x_i) = \frac{exp(\beta_0+\beta_1x_i)}{1+exp(\beta_0+\beta_1x_i)}=\frac{1}{1+exp(-\{\beta_0+\beta_1x_i\})}$

- $\log(\frac{\theta(x_i)}{1-\theta(x_i)})=\beta_0+\beta_1x_i$ where $\log(\frac{\theta(x_i)}{1-\theta(x_i)})$ is called logit.

- $\log(\frac{\hat{\theta(x_i)}}{1-\hat{\theta(x_i)}})=\hat{\beta_0}+\hat{\beta_1}x_i$

- We use maximum likelihood estimates for coefficients, but no closed-form solutions exist. Thus, we take advantage of computation results.

- "For every unit increase in X, the odds that the characteristic is present is multiplied by $exp(\beta_1)$."

- Recall the PDF of binomial: $P(Y_i=y_i|x_i) = {m_i\choose y_i} \theta(x_i)(1-\theta(x_i))^{m_i-y_i}$ 

- Recall the log likelihood of binomial : $\log(L)=\sum^n_{i=1}(y_i(\beta_0+\beta_1x_i)-m_i\log(1+exp(\beta_0+\beta_1x_i))+\log{m_i\choose y_i})$.

#### Goodness of fit test

- $H_o:$ The logistic regression is appropriate vs. $H_a:$ Not appropriate.

- We use two log likelihood : $\log(L_M)$ vs. $\log(L_S)$ where M refers the logistic regression model and S refers to the saturated model, a model with a theoretically perfect fit. Thus, $\hat{y_i}=y_i$ under the saturated model.

- Set $\hat{y_i}=\hat{\theta_M}(x_i)m_i$

1. Deviance: $G^2=2(\log(L_S)-\log(L_M))=2\sum^n_{i=1}[y_i\log(\frac{y_i}{\hat{y_i}})+(m_i-y_i)\log(\frac{m_i-y_i}{m_i-\hat{y_i}})]\sim\chi^2_{n-p-1}$. IF the model is appropriate, then $G^2$ is smaller so we fail to reject the null. $\text{P-value}=P(\chi^2_{n-p-1}>G^2_{obs})$

2. Pearson $\chi^2$ statistic: $\chi^2=\sum\frac{(y_i/m_i-\hat{\theta}(y_i))^2}{\hat{\theta}(y_i)(1-\hat{\theta}(y_i))/m_i}\sim\chi^2_{n-p-1}$

3. R-squared: $R^2_{dev} = 1-\frac{G^2_{H_a}}{G^2_{H_o}}$

#### Comparing models

- $H_o: \theta(x)=\frac{1}{1+exp(-\beta_0)}$ vs. $H_a: \theta(x)=\frac{1}{1+exp(-\{\beta_0+\beta_1x\})}$

- $G^2_{H_o}-G^2_{H_a}\sim\chi^2_{df_1-df_2}$ where $df_1=n-(\text{num. of predictors in } H_0)-1$ and $df_2=n-(\text{num. of predictors in } H_a)-1$

#### Marginal model plot

Compare the following two models, and determine wheather the logistic regression is appropriate. If two models are significantly different, then the logistic regression ins not appropriate.

1. Parametric model : $\theta(x_i)=\frac{1}{1+exp(-\{\beta_0+\beta_1x_i\})}$ 

2. Nonparametric model : $\theta(x) = f(x_1,\dots,x_p)$. For the model with $p$ predictors, we need $p$ many marginal model plots. If any of them show discrepancy from parametric model, then the parametric model (logistic regression) is not appropriate.


\newpage

#### R: Logistic Regression

- Some predictor variables cause an error:`glm.fit: algorithm did not converge` when the sample size is not enough.

- If `Gdiff < 0.05`, reject $H_o$ and full model is better.

\ 

```{r}
m = glm(am~mpg+disp+hp+wt, family=binomial(), data=mtcars)
summary(m)

Gdiff = m$null.deviance-m$deviance
pchisq(Gdiff,4,lower=FALSE)

library(alr3)
mmps(m)
```

\newpage

#### R: Comparing models

- Here, failed to reject Null, `m3` model is better.

- Also, one of the plot shows a descrepancy. Logistic regression for `m2` might not be appropriate.

\ 

```{r}
m3 = glm(am~mpg+hp+wt, family=binomial(), data=mtcars)
mmps(m3)

m2 = glm(am~hp+wt, family=binomial(), data=mtcars)
mmps(m2)

anova(m3,m2,test="Chisq")
```

\newpage

#### R: Prediction

\ 

```{r}
set.seed(1)
train.i = traindex(mtcars, trainn = 25)

Y_train = mtcars[train.i, c("am")]
Y_test = mtcars[-train.i, c("am")]
X_train = mtcars[train.i, c("mpg","hp","wt")];X_train = cbind(X_train,Y_train)
X_test = mtcars[-train.i, c("mpg","hp","wt")]

m = glm(Y_train~mpg+hp+wt, family = binomial(), data=X_train)
Y_pred = ifelse(predict(m, X_test, type="response")>=0.5,1,0)
table(Y_test, Y_pred)
```


\newpage

## Bayes Classifier

- $P()=\frac{P()P()}{P()}$


\newpage

## K Nearest Neighbor

- Non-probablistic, supervised learning, classification

- Judge the category of a point via the category of K many Euclidian nearest others.

- No assumption on distribution of X, but they have to be numeric continuous.

- Draw decision boundary according to K many closest neighbors based on Euclidian distance.

- Make sure to standarize the predictor variable X because the algorithm takes Euclidan distance to determine the line, and standarizing increase the accuracy of the results.

- Highly non-linear boundry for smaller K.

---

```{r}
library(class)
set.seed(1)
test.i = sample(1:nrow(Heart),50, replace=F)

# Standarize predictors (continuous)
Xs = scale(cbind(Heart$Age, Heart$MaxHR))
Xs_test = Xs[test.i,]
Xs_train = Xs[-test.i,]
Y_test = Heart$AHD[test.i]
Y_train = Heart$AHD[-test.i]
knn_output = knn(Xs_train, Xs_test, Y_train, k=1)

# MSE
mean(knn_output!=Y_test)

# Confusion Matrix
table(knn_output, Y_test)
```

\newpage 

#### Find the best K

\ 

```{r}
library(caret)
library(e1071)
Heart_train = cbind(Xs_train,Y_train);colnames(Heart_train) = c("Age","MaxHR","AHD")
ctrl = trainControl(method="repeatedcv", repeats=13)
knnfit = train(as.factor(AHD)~., data=Heart_train, trControl=ctrl, method="knn", preProcess = c("center","scale"), tuneLength = 10)
knnfit
knnfit_val = data.frame(knnfit[4])
plot(knnfit_val[,1], knnfit_val[,2], type="b", col="blue", xlab="k", ylab="Accuracy")
maxacu = which(knnfit$results$Accuracy==max(knnfit$results$Accuracy));maxacu
```

\newpage

#### Repeat with the best K

\ 

```{r}
knn_output = knn(Xs_train, Xs_test, Y_train, k=maxacu, prob=T)
# MSE
mean(knn_output!=Y_test)
# Confusion Matrix
table(knn_output, Y_test)
```

\newpage

## Linear Descriminant Analysis

- non-probablistic, supervised learning, classification.

- Finds the best place to make the best split linear boundary between two (or more) distributions.  Relies on the Bayes Classifier. Tries to find the K-dimenstional projection that creates the greatest between group separation.

- Assumption: Normaility and small sample size, and same variance among all group (cateogry).

- Make the dataset linearly independent. LD1, LD2... are the coefficients such that makes the dataset linearly independent i.e. eigenvectors.

- Dimension reduction.

- The devision line $x=\frac{\mu_1+\mu_2}{2}$

```{r}
ggplot(iris) + geom_density(mapping=aes(x=Sepal.Length, color=as.character(Species)))+ geom_vline(xintercept = 5.42)
```

- The lda$svd represents the eigenvalues, and bigger the better split the data.

- LD1 and LD2 on the bottom represents the trace. Bigger the more important role.

- LD1, LD2 with coefficients are the eigenvectors. Make sure that they are all standarized.

---

```{r}
library(MASS)
set.seed(1)
test.i = sample(1:nrow(iris),30, replace=F)
X_train = iris[-test.i, 1:4]
X_test = iris[test.i, 1:4]
Y_train = iris[-test.i, 5]
Y_test = iris[test.i, 5]

# LDA
model_lda = lda(Y_train~X_train$Sepal.Length+X_train$Sepal.Width);model_lda
# Take LD1(The first split)
LD1 = predict(model_lda)$x[,1]

plot(Y_test)
plot(LD1, type="n");text(LD1,labels=unclass(iris$Species))
```

\newpage

```{r}
m3 = lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)
m3
LD1<-predict(m3)$x[,1]
LD2<-predict(m3)$x[,2]
plot(LD1,LD2,xlab="first linear discriminant",ylab="second linear discriminant",type="n")
text(cbind(LD1,LD2),labels=unclass(iris$Species))

head(2.105107+0.8293776*iris$Sepal.Length+1.5344731*iris$Sepal.Width-2.2012117*iris$Petal.Length-2.8104603*iris$Petal.Width)

head(LD1)
cor(iris[,1],LD1)

m3$svd


iris.lda<-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,  data = iris)
iris.lda
plot(iris.lda)

iris.lda$svd
iris.lda$counts
iris.lda$means
iris.lda$lev

# Plots:
LD1<-predict(iris.lda)$x[,1]
LD2<-predict(iris.lda)$x[,2]

plot(LD1,LD2,xlab="first linear discriminant",ylab="second linear discriminant",col=2:4,type="n",main="LDA1 vs LDA2 for the three Species")
text(cbind(LD1,LD2),labels=iris$Species)
legend(0.5,2.8,legend=c("Setosa","Versicolor","Virginica"))

plot(LD1,LD2,xlab="first linear discriminant",ylab="second linear discriminant",col=2:4,type="n",main="LDA1 vs LDA2 for the three Species")
text(cbind(LD1,LD2),labels=unclass(iris$Species))
legend(0.5,2.8,legend=c("1=Setosa","2=Versicolor","3=Virginica"))
# 1="setosa"     
# 2="versicolor" 
# 3="virginica" 

qplot(x = LD1, y = LD2, colour = iris$Species,shape = iris$Species,main="LDA1 vs LDA2 for the three Species")

# Group centroids

sum(LD1*(iris$Species=="setosa"))/sum(iris$Species=="setosa")
sum(LD2*(iris$Species=="setosa"))/sum(iris$Species=="setosa")

sum(LD1*(iris$Species=="versicolor"))/sum(iris$Species=="versicolor")
sum(LD2*(iris$Species=="versicolor"))/sum(iris$Species=="versicolor")

sum(LD1*(iris$Species=="virginica"))/sum(iris$Species=="virginica")
sum(LD2*(iris$Species=="virginica"))/sum(iris$Species=="virginica")

iris.predict<-predict(iris.lda,iris[,1:4])
iris.classify<-iris.predict$class
iris.classperc<-sum(iris.classify==iris[,5])/150
iris.classperc

table(Original=iris$Species,Predicted=predict(iris.lda)$class)


```

\newpage

## Quadratic Discriminant Analysis

- In stead of linear boundary, we use non-linear boundary.


```{r}
m4 = qda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris)
m4
```



\newpage

## Support Vector Machine

\newpage

## K-Means Clustering


\newpage

## Kernelized Clustering

\newpage

## EM Type Algorithm

\newpage

## Decision Trees

- Partition data points. Determine the value of response variable (if continuous, the mean of the response) according to which partition that new predictor belongs to (looks like a house diagram). If Y is continuous, it is called regression tree and if Y is cateogorical, it is called classification tree.

- Determine the splits lines that generate the lowest MSE. As we have more splits, the new value of Y can be determined on the tree structure, or "nested if" structure.

- Pruning tree: A large tree can be over-fitting, thus pruning tree allows cutting off some of the terminal nodes. Find the best pruning by cross validation.

- If the relationship between the predictors and response is linear, then linear regression is better; if the relationship is "rectangle" shapes, tree performs better.

- Easily interpretable (e.g. If weight is above 40kg, height is higher than 140cm in average). However, suffer from high variance. Thus, use random forest to resolve this issue.

---

#### R: Tree

\ 

```{r}
library(tree)
train.i = traindex(iris)
iris_train = iris[train.i,]
iris_test = iris[-train.i,1:4]
Y_test = factor(iris[-train.i,5])

tree1 = tree(Species~., data=iris_train)
summary(tree1)
plot(tree1);text(tree1)

# Prediction
preds=predict(tree1,newdata=iris_test, type="class")
table(Y_test, preds)
```

---

#### R: Pruning Tree

- Here, it shows that pruning with 5 branches is appropriate.

\ 

```{r}
cv.train=cv.tree(tree1,FUN=prune.misclass)
plot(cv.train$dev~cv.train$size)
pruned.fit=prune.misclass(tree1,best=5)

plot(pruned.fit)
text(pruned.fit,pretty=TRUE)
summary(pruned.fit)

# Prediction
pred.prune=predict(pruned.fit,newdata=iris_test,type="class")
table(Y_test, pred.prune)
```

---

#### R: Regression Tree

\ 

```{r}
mtcars_temp = subset(mtcars,select=c(mpg,wt, disp, qsec))
tree2=tree(mpg~., data=mtcars_temp)
summary(tree2)
plot(tree2)
text(tree2,pretty=0)
```

\newpage

## Random Forest

- Use bagging: Bootstrap + averaging. That is, generate B different bootstrapped training dataset. Train the statistical learning method on each of the B training datasets, and obtain the prediction, then take the average. In this case, construct B different trees using B bootstrapped dataset, then take the average of the results.

- If continuous, average all predictions from all B trees. If Classification, majority vote among all B trees. These trees are not pruned, so each individual tree has high variance but low bias. Averaging these trees reduces variance, and thus lowering both variance and bias can be achieved.

- Two methods for prediction: Record the class that each bootstrapped data set predicts and provide an overall prediction to the most commonly occurring one (majority vote). Or, if our classifier produces probability estimates we can just average the probabilities and then predict to the class with the highest probability.

- Bagging improves prediction accuracy at the expense of interpretability. But, we can use relative influence plots to see the contributions of each variables to the model. Larger the more influential.

- Random forest: Build a number of decision trees on bootstrapped training sample, but when building these trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. Only m predictors are used for the sake of "de-correlation" of the model; if all variables are used, each models for each bootstrapped dataset will be similar to each other, hence highly correlated. Averaging many highly correlated quantities does not lead to a large variance reduction.

---

#### R: RF, `mtry=4`

```{r}
library(randomForest)

RF=randomForest(Species~.,data=iris_train,mtry=4,importance=TRUE, ntree=100)
summary(RF)
plot(RF)
print(RF)
importance(RF)
varImpPlot (RF)

preds = predict(RF,newdata=iris_test)
table(Y_test, preds)
```

---

#### R: RF, `mtry=2`

```{r}
RF=randomForest(Species~.,data=iris_train,mtry=2,importance=TRUE, ntree=100)
summary(RF)
plot(RF)
print(RF)
importance(RF)
varImpPlot (RF)

preds = predict(RF,newdata=iris_test)
table(Y_test, preds)
```

\newpage

## Neural Network

\newpage

## Comparing all classification methods


|Method|Linearity|Normality|Constant Variance|Big Sample Size|K>2|
|-------------------|---------|---------|-----------------|---------------|---|
|Logistic Regression|Yes|Yes on Y|Yes|Yes|K=2|
|LDA|Yes|Yes on X and Y|Yes|No|Yes|
|QDA|No|Yes|No|No|Yes|
|KNN|No|No|No|Yes|Yes|

\newpage

# Experimental Design

\newpage

# Computational Techniques

## Standarization / Normalization

\newpage

## K-Fold Cross Validation

- A method to validate the model and its parameters. MSE is highly variable measurement for the model. Thus, cross validation reduce this variablity and help us obtain the most stable MSE.

- Devide the data set into K different parts. Remove the first part, fit a model using the rest of the parts, and test on this removed first part and take MSE. Repeat this procedure for all folds. At last, average all K different MSE and obtain the MSE for the model.

- When $K=n$, it's called "Leace-One-Out Cross Validation".

---

#### R: CV to choose order of polynomial

```{r}
library(boot)
set.seed(1)
K = 5
CV_MSE = rep(0,K)
for(i in 1:K){
  glm.fit = glm(Sepal.Width~poly(Sepal.Length,i), data=iris)
  CV_MSE[i] = cv.glm(iris, glm.fit)$delta[1]
}
CV_MSE
```


\newpage

#### R: Package "crossval"

```{r}
library(crossval)
```

\newpage

## Bootstrap


#### R: Obtain mean and CI using package "boot"

\ 

```{r}
library(boot)
set.seed(1)

# Mean
mean.fn = function(data, index){
return(c(mean(data[index,1])))
}
mean_boot = boot(iris, mean.fn, R=1000)

# CI

CI_norm_boot = boot.ci(mean_boot, type = 'norm')
CI_norm_boot$normal



```


\newpage

# R Features

## Basics

## Functions

\newpage

### `I()` and `poly()` for polynomial regression

- `I()` function simply grants `lm()` function to add higher order variables.

- `poly()` function uses an orthogonal basis to fit polynomial regression. That is, the variables are linearly transformed into linearly independent set, thus no multicolinearity is present. Moreover, the statistics generated such as r^2 and resulting plots are the same as using `I()` functions. However, the coefficients generated are different hence it suffers from interpretations.

---

```{r}
grids=seq(from=min(iris$Sepal.Length),to=max(iris$Sepal.Length), by=0.01)

m_I = lm(Sepal.Width~Sepal.Length+I(Sepal.Length^2), data=iris)
summary(m_I)
pred = predict(m_I,newdata=list(Sepal.Length=grids),se=TRUE)
plot(Sepal.Width~Sepal.Length, data=iris);lines(grids,pred$fit, lwd=2, col="blue")

m_poly = lm(Sepal.Width~poly(Sepal.Length,2), data=iris)
summary(m_poly)
pred = predict(m_poly,newdata=list(Sepal.Length=grids),se=TRUE)
plot(Sepal.Width~Sepal.Length, data=iris);lines(grids,pred$fit, lwd=2, col="blue")
```

\newpage

### Normalization and Standarization

### `apply()`, `sapply()`, `tapply()`, `lapply()`

### Package: `dplyr`






